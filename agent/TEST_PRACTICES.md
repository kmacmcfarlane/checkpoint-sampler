# TEST_PRACTICES.md 

This document defines how tests are written, organized, and run across backend (Go) and frontend (Vue/TypeScript). It is a contract: stories are not “done” unless tests meet these practices.

## 1) Principles

### 1.1 Tests are a product requirement
- Tests encode acceptance criteria.
- A story is incomplete without tests unless the story explicitly waives them.

### 1.2 Determinism
- Tests must be deterministic and fast.
- No sleeps as synchronization. Use proper coordination (channels, wait groups, fake clocks).

### 1.3 No real network / no real sends
- Unit tests must not call external networks.
- No test may make real external calls.
- Provider integrations are mocked/stubbed.

### 1.4 Data-driven tests by default when repeated patterns exist
- Prefer table-driven tests to reduce duplication and improve coverage.
- Backend: Ginkgo `DescribeTable` / `Entry`
- Frontend: `it.each` / parameterized helper loops

### 1.5 Test pyramid
- Unit tests: majority
- Service/API tests: targeted and meaningful
- UI/component tests: focused on behavior and critical paths

## 2) Backend (Go) testing practices

### 2.1 Frameworks and tooling
- Use Ginkgo + Gomega.
- Use race detection where feasible (`--race`).
- Use mocks generated by mockery for interfaces.
- Avoid heavy integration dependencies in unit tests.

### 2.2 Test layout and naming
- Place tests adjacent to packages under:
    - `backend/internal/...`
    - `backend/pkg/...` (if used)
    - `backend/cmd/...` (rare; keep minimal)
- Name files `*_test.go`.
- Organize specs with consistent structure:
    - `Describe("<subject>")`
    - `Context("<scenario>")`
    - `It("does <behavior>")`

### 2.3 Preferred test structure (arrange/act/assert)
- Arrange:
    - Build inputs and fakes.
    - Set up store/service with mocks.
- Act:
    - Call the unit under test.
- Assert:
    - Use Gomega expectations.
    - Verify returned values and side effects.
    - Verify mock calls when behavior depends on collaborators.

### 2.4 Data-driven tests (required when appropriate)
Use `DescribeTable` when:
- Testing validation rules over multiple inputs
- Testing error mapping or status transitions
- Testing selection logic across multiple cases

Example pattern (illustrative):
- `DescribeTable("validates inputs", func(tc testCase) { ... }, Entry("case", tc1), Entry("case", tc2))`

### 2.5 What to mock vs what to test “for real”
Mock:
- External provider clients
- Time
- Randomness / UUID generation
- External IO beyond the database (filesystems, OS calls)

Test for real:
- Domain logic in `internal/service`
- Mapping logic between domain model and persistence entities
- Schema constraints and migrations (with temporary DB files)

### 2.6 Database testing guidelines
- Use temporary DB files or in-memory databases only when they reflect production behavior.
- Prefer temp file DB for migration/schema tests to match real usage.
- Ensure tests clean up resources (file deletion).
- Verify critical constraints:
    - foreign keys (if enabled)
    - unique indexes
    - timestamps and default behaviors

### 2.7 API and transport tests (Goa)
- Test the service layer first; transport tests are secondary.
- For transport:
    - Validate status codes and response shapes for key endpoints.
    - Ensure auth gating is enforced.
    - Ensure errors are mapped to stable error codes/messages.

### 2.8 Security tests
If authentication or encryption is implemented:
- Password hashing verification (correct password succeeds, wrong fails)
- Encryption round-trip (encrypt then decrypt yields original, wrong key fails)
- Ensure secrets do not appear in logs (where log output is testable)

### 2.9 Error handling tests
- Every exported service method must have at least:
    - a happy-path test
    - a failure-path test
- Error objects should expose stable error codes for UI consumption; verify codes.

### 2.10 Running tests
Baseline expectations:
- `make test` runs the backend test suite.
- Watch mode:
    - `make test-backend-watch` runs `ginkgo watch` (race where feasible).
- CI-like run (when available):
    - `ginkgo -r --race ./internal/... ./pkg/... ./cmd/...`

### 2.11 Coverage expectations
- No hard numeric coverage gate is required initially.
- Practically:
    - service layer and schema logic should have high coverage
    - UI routing glue and generated code are excluded
- Any uncovered critical logic must be justified in the story notes.

## 3) Frontend (Vue + TypeScript) testing practices

### 3.1 Frameworks and tooling
- Use Vitest.
- Prefer Vue Testing Library patterns for user-centric behavior when possible.
- Do not rely on brittle DOM structure; assert on user-visible outcomes.

### 3.2 Test types
- Component tests:
    - forms (CRUD operations)
    - interactive UI behaviors
    - workflow sequences
- Store/state tests:
    - data fetching and normalization
    - error state handling
- Utility tests:
    - input validation
    - formatting utilities

### 3.3 Data-driven tests (required when appropriate)
Use parameterization when:
- validating multiple input cases
- testing transformation functions
- testing list filtering behavior
- testing error presentation for multiple backend error codes

Preferred patterns:
- `it.each([...cases])("...", (tc) => { ... })`
- Or a loop over cases with a shared helper.

### 3.4 Mocking strategy
Mock:
- backend API calls (never call real backend in unit tests unless explicitly building an integration test suite)
- browser APIs not supported in test environment
- time where relevant

For API calls:
- Provide a thin API client module and mock it in tests.
- Ensure errors and status codes map to user-visible messages and states.

### 3.5 UI assertions
Prefer:
- visible text
- aria roles/labels
- button enabled/disabled state
- emitted actions (e.g., API calls made with correct payload)

Avoid:
- asserting implementation details of UI libraries
- deep snapshot tests for large components (allowed only for small stable components)

### 3.6 Running tests
- `npm run test` runs non-watch suite (if present).
- Watch mode is required:
    - `npm run test:watch`
    - Root target `make test-frontend-watch` runs this inside docker dev workflow.

### 3.7 Auto-unmount
All frontend test files must use `enableAutoUnmount(afterEach)` from `@vue/test-utils`, either in each test file or in a shared vitest setup file. This prevents test interference from stale event listeners when `wrapper.unmount()` is not called explicitly.

### 3.8 localStorage isolation
A global `beforeEach` in the vitest setup file must call `localStorage.clear()` to prevent cross-test contamination. Do not rely on individual test files to clear localStorage.

### 3.9 Nested component mock ordering
When a parent and child component both call the same API on mount, the test mock setup must account for both calls in the correct order. Document the expected call sequence in test comments when mock ordering is non-obvious.

### 3.10 Naive UI prop casing
Naive UI template attributes use kebab-case (e.g., `consistent-menu-width`) but Vue props in test assertions use camelCase (e.g., `consistentMenuWidth`). Always use camelCase when asserting on component props in tests.

### 3.11 Security Audit
If `npm audit` shows any high-severity vulnerabilities, fail the QA cycle and have the developer upgrade the vulnerable packages.

## 4) Cross-cutting test policies

### 4.1 Fixtures and test data
- Keep fixtures small and explicit.
- Prefer builders for complex objects.
- Use deterministic IDs/timestamps via injection.

### 4.2 Logging
- Tests must not assert on full log text unless validating absence of secrets or presence of stable error codes.
- Never store real credentials in fixtures.

### 4.3 Flake policy
- No retries to mask flakiness.
- Fix flakiness at the source:
    - eliminate timing assumptions
    - isolate shared state
    - avoid global mutable singletons

### 4.4 When to add integration tests
Add targeted integration tests when:
- validating DB migrations end-to-end
- validating API routing/auth middleware for critical endpoints

Integration tests must still avoid real provider calls.

## 5) QA subagent integration

The QA expert subagent (/.claude/agents/qa-expert.md) is the final gate before a story is marked `status: uat`. It verifies:

### 5.1 Test execution
- All backend tests pass (`make test-backend`)
- All frontend tests pass (`make test-frontend`)
- Zero test failures

### 5.2 Acceptance traceability
- Each acceptance criterion in the story is traced to a specific test or verified code path
- Edge cases implied by acceptance criteria are covered
- Tests are meaningful (not trivially passing)

### 5.3 Coverage assessment
- New/changed behaviors have tests following the practices in this document
- Data-driven tests used where patterns repeat
- Happy-path and failure-path coverage for new service methods
- No real network calls in any test

### 5.4 Regression verification
- All pre-existing tests still pass
- No functionality broken by the changes

### 5.5 E2E smoke test (PRIMARY — REQUIRED)

E2E tests are the standard verification method for story acceptance. The QA expert must run the full Playwright E2E suite as the primary smoke test mechanism for every story:

```
make test-e2e
```

`make test-e2e` is self-contained: it starts backend + frontend using the E2E compose stack with `test-fixtures/` data, runs all Playwright tests, then tears down automatically. No separate `make up-dev` is required.

This verifies that the application starts, serves requests, and that user-facing behavior is correct end-to-end. A passing E2E run confirms:
- The application starts without fatal errors (crash loops, missing dependencies, broken wiring)
- The backend health endpoint responds successfully
- User journeys exercised by the test suite work correctly across the full stack

**When E2E coverage is insufficient for the story's acceptance criteria**, the QA expert must write or update E2E tests to cover those criteria before approving. New or modified E2E tests must pass before the story is approved.

Results (tests run, passed, failed) must be recorded in the "E2E Test Results" section of the QA verdict.

### 5.6 Manual HTTP verification (secondary — debugging aid only)

Manual curl/HTTP checks are a debugging tool, not a substitute for E2E tests. Use them only when:
- Investigating a specific failure observed in E2E or unit tests
- Authoring a new E2E test and needing to confirm raw API behavior first
- Diagnosing a startup or connectivity issue before E2E tests can run

If a story's acceptance criteria require API verification, write an E2E test or extend an existing one rather than relying on manual curl commands as the acceptance gate.

Notes on sandbox connectivity (for debugging use):
- When running inside the claude-sandbox, `localhost` ports mapped to the host are not directly reachable. Use `docker compose exec <service> wget -qO- http://localhost:<port>/health` or similar within-network commands.
- The canonical health check path is `/health` (direct backend endpoint).
- For API endpoints, issue requests via `curl` from within the docker network or use the Playwright `request` fixture in E2E tests.

The QA expert may return a story to `in_progress` with specific issues recorded in the story's `review_feedback` field. The fullstack engineer must address all `blocker` and `important` severity issues before re-submitting.

### 5.7 Runtime error sweep (secondary findings)

After completing section 5.5 (E2E smoke test), the QA expert must perform a runtime error sweep on the application logs. This sweep is a **secondary check** — its results do NOT block the current story. If the story's acceptance criteria pass, the story is APPROVED regardless of sweep findings.

#### 5.7.1 Procedure

1. Capture logs. Two options:
   a. Use the E2E logs captured automatically by `make test-e2e`. After a run, logs are written to:
      - `.ralph-temp/e2e-logs/backend.log`
      - `.ralph-temp/e2e-logs/frontend.log`
      Review these files directly. This is the preferred option when E2E tests have been run.
   b. Start the application briefly, capture logs, then tear down:
      ```
      make up-dev
      docker compose logs --tail=500 --no-color 2>&1
      make down
      ```
2. Filter for error-level and fatal-level messages:
   ```
   grep -iE 'level=(error|fatal|panic)|FATAL|PANIC|panic:'
   ```
3. Read `/agent/QA_ALLOWED_ERRORS.md` for the expected error allowlist.
4. For each error line found, classify it as one of:
   - **Expected**: Matches a pattern in `QA_ALLOWED_ERRORS.md`. Skip it.
   - **Bug**: An unexpected runtime error that indicates a defect. Report as a new bug ticket.
   - **Improvement**: A non-critical issue suggesting a code improvement (e.g., noisy logging for recoverable conditions). Report as an improvement idea for the appropriate file under `/agent/ideas/`.
5. Report findings in the QA verdict using the structured format (see qa-expert.md).

#### 5.7.2 Bug ticket fields

For each finding classified as **Bug**, the QA agent reports:
- A brief title (suitable for a backlog ticket)
- The error log line(s) that triggered the finding
- A brief root cause hypothesis (1-2 sentences)
- Suggested acceptance criteria (1-3 items)
- Suggested testing commands
- Suggested priority (default: 70)

#### 5.7.3 Non-blocking rule

Sweep findings never affect the story verdict. If the story's acceptance criteria pass, the story is **APPROVED** and sweep findings are filed as separate tickets by the orchestrator. This prevents infinite loops where every story bounces for pre-existing issues.

### 5.8 E2E test execution details

Section 5.5 establishes E2E tests as the primary smoke test. This section provides additional details on E2E execution and failure triage.

Key facts:
- `make test-e2e` is fully self-contained: it starts backend + frontend using `docker-compose.e2e.yml` with `test-fixtures/` data, runs all Playwright tests, and tears down automatically. No separate `make up-dev` is required.
- The E2E compose project (`checkpoint-sampler-e2e`) is isolated from the dev environment — running `make test-e2e` does not interfere with an active `make up-dev` session.

**Story-related E2E failures (blocking):**
- If a failing test covers a user journey touched by this story's changes, the failure is blocking. Investigate, fix the code or update the test, and ensure it passes before approving the story.

**Pre-existing / unrelated E2E failures (non-blocking):**
- If a failing test covers a user journey NOT touched by this story, do not reject the story. Record the failure and file it as a bug ticket via the runtime error sweep mechanism.

## 6) End-to-End (E2E) testing practices — Playwright

### 6.1 Frameworks and tooling
- Use `@playwright/test` (installed as a dev dependency in `/frontend`).
- Configuration lives at `frontend/playwright.config.ts`.
- E2E test files live at `frontend/e2e/` with the `*.spec.ts` extension.
- Run with `make test-e2e` — fully self-contained, no separate stack required.

### 6.2 Environment
- Base URL: `http://frontend:3000` (Vite dev server inside the E2E compose stack).
- Use headless Chromium. In the claude-sandbox, `chromiumSandbox: false` and `--no-sandbox` args are required.
- `make test-e2e` is self-contained: it starts backend + frontend using `docker-compose.e2e.yml` with
  `test-fixtures/` data, runs Playwright, then tears down. No separate `make up-dev` needed.
- The E2E compose project is named `checkpoint-sampler-e2e` — isolated from `make up-dev`.
- Test fixture data lives in `test-fixtures/`: deterministic checkpoint and sample data for reproducible E2E tests.
- Use `make up-test` / `make down-test` for an isolated test stack with separate Docker volumes and a
  separate project name (`checkpoint-sampler-test`). Resetting with `make down-test` does not affect
  the `make up-dev` environment.

### 6.3 What belongs in E2E tests
- Critical user journeys that span the full stack (UI, API, DB).
- Smoke tests that verify the app loads and the backend is reachable.
- Flows that cannot be validated by unit or component tests alone (navigation, real API responses).

### 6.4 What does NOT belong in E2E tests
- Business logic that is already covered by unit or component tests.
- Exhaustive validation of every input combination (use unit tests for that).
- Anything requiring network calls outside the project (all external calls must be internal to the test stack).

### 6.5 Writing E2E tests
- Use `page.goto('/')` and Playwright locators; prefer role-based or `data-testid` selectors.
- Use `request.get('/health')` (via the `request` fixture) to verify API endpoints through the proxy.
- Avoid arbitrary `page.waitForTimeout()` calls; use `expect(...).toBeVisible()` and Playwright's auto-waiting instead.
- Keep each spec file focused on a single feature or user journey.
- The smoke test in `frontend/e2e/smoke.spec.ts` is the canonical example.

**AC-to-test traceability**
Tests should include inline comments linking to the acceptance criteria they verify. Format: `// AC: <acceptance criterion text or summary>`. This makes QA traceability verification faster. Both unit tests and E2E tests should follow this pattern.

### 6.6 Running E2E tests
- Agent workflow (one-shot): `make test-e2e` (self-contained; starts the stack, runs tests, tears down).
- Playwright browsers are installed in the project's local `node_modules` — no global install needed.
- Playwright outputs results in list reporter format for easy log scanning.

### 6.7 Selector stability
Never use Naive UI internal CSS classes (e.g., `.n-dynamic-tags__add`, `.n-select-option`) in E2E selectors — these can change between library versions. Always use `data-testid` attributes on interactive elements. When wrapping Naive UI components, add `data-testid` to the wrapper's interactive sub-elements.

### 6.8 E2E timeout configuration
Set an explicit `timeout` in `playwright.config.ts` (e.g., `timeout: 15000`) so the value is visible and intentional. Do not rely on Playwright's default timeout.

### 6.9 NDrawer mask interaction
NDrawer renders a mask overlay that intercepts pointer events on elements behind it. In E2E tests, always close or collapse the drawer before clicking on grid cells or other elements underneath it. Add a brief `page.waitForTimeout(300)` after closing to allow the mask animation to complete.

### 6.10 Playback test timing
For timing-sensitive E2E assertions (e.g., slider playback advancement), set playback speed to the minimum value and use generous timeouts (e.g., `{ timeout: 5000 }` for a 0.25s speed gives a 20x safety margin). Avoid `page.waitForTimeout()` except for hold-position verification where you need to assert the slider has NOT moved.

## 7) Definition of Done (testing)
A story may be set to `status: uat` only when:
- New/changed behavior is covered by tests following these practices.
- Tests are deterministic and fast enough for watch workflows.
- All relevant suites pass locally and in docker watch mode (when applicable).
- The QA expert subagent has approved the story (see section 5).
