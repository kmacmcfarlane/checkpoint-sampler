# TEST_PRACTICES.md 

This document defines how tests are written, organized, and run across backend (Go) and frontend (Vue/TypeScript). It is a contract: stories are not “done” unless tests meet these practices.

## 1) Principles

### 1.1 Tests are a product requirement
- Tests encode acceptance criteria.
- A story is incomplete without tests unless the story explicitly waives them.

### 1.2 Determinism
- Tests must be deterministic and fast.
- No sleeps as synchronization. Use proper coordination (channels, wait groups, fake clocks).

### 1.3 No real network / no real sends
- Unit tests must not call external networks.
- No test may make real external calls.
- Provider integrations are mocked/stubbed.

### 1.4 Data-driven tests by default when repeated patterns exist
- Prefer table-driven tests to reduce duplication and improve coverage.
- Backend: Ginkgo `DescribeTable` / `Entry`
- Frontend: `it.each` / parameterized helper loops

### 1.5 Test pyramid
- Unit tests: majority
- Service/API tests: targeted and meaningful
- UI/component tests: focused on behavior and critical paths

## 2) Backend (Go) testing practices

### 2.1 Frameworks and tooling
- Use Ginkgo + Gomega.
- Use race detection where feasible (`--race`).
- Use mocks generated by mockery for interfaces.
- Avoid heavy integration dependencies in unit tests.

### 2.2 Test layout and naming
- Place tests adjacent to packages under:
    - `backend/internal/...`
    - `backend/pkg/...` (if used)
    - `backend/cmd/...` (rare; keep minimal)
- Name files `*_test.go`.
- Organize specs with consistent structure:
    - `Describe("<subject>")`
    - `Context("<scenario>")`
    - `It("does <behavior>")`

### 2.3 Preferred test structure (arrange/act/assert)
- Arrange:
    - Build inputs and fakes.
    - Set up store/service with mocks.
- Act:
    - Call the unit under test.
- Assert:
    - Use Gomega expectations.
    - Verify returned values and side effects.
    - Verify mock calls when behavior depends on collaborators.

### 2.4 Data-driven tests (required when appropriate)
Use `DescribeTable` when:
- Testing validation rules over multiple inputs
- Testing error mapping or status transitions
- Testing selection logic across multiple cases

Example pattern (illustrative):
- `DescribeTable("validates inputs", func(tc testCase) { ... }, Entry("case", tc1), Entry("case", tc2))`

### 2.5 What to mock vs what to test “for real”
Mock:
- External provider clients
- Time
- Randomness / UUID generation
- External IO beyond the database (filesystems, OS calls)

Test for real:
- Domain logic in `internal/service`
- Mapping logic between domain model and persistence entities
- Schema constraints and migrations (with temporary DB files)

### 2.6 Database testing guidelines
- Use temporary DB files or in-memory databases only when they reflect production behavior.
- Prefer temp file DB for migration/schema tests to match real usage.
- Ensure tests clean up resources (file deletion).
- Verify critical constraints:
    - foreign keys (if enabled)
    - unique indexes
    - timestamps and default behaviors

### 2.7 API and transport tests (Goa)
- Test the service layer first; transport tests are secondary.
- For transport:
    - Validate status codes and response shapes for key endpoints.
    - Ensure auth gating is enforced.
    - Ensure errors are mapped to stable error codes/messages.

### 2.8 Security tests
If authentication or encryption is implemented:
- Password hashing verification (correct password succeeds, wrong fails)
- Encryption round-trip (encrypt then decrypt yields original, wrong key fails)
- Ensure secrets do not appear in logs (where log output is testable)

### 2.9 Error handling tests
- Every exported service method must have at least:
    - a happy-path test
    - a failure-path test
- Error objects should expose stable error codes for UI consumption; verify codes.

### 2.10 Running tests
Baseline expectations:
- `make test` runs the backend test suite.
- Watch mode:
    - `make test-backend-watch` runs `ginkgo watch` (race where feasible).
- CI-like run (when available):
    - `ginkgo -r --race ./internal/... ./pkg/... ./cmd/...`

### 2.11 Coverage expectations
- No hard numeric coverage gate is required initially.
- Practically:
    - service layer and schema logic should have high coverage
    - UI routing glue and generated code are excluded
- Any uncovered critical logic must be justified in the story notes.

## 3) Frontend (Vue + TypeScript) testing practices

### 3.1 Frameworks and tooling
- Use Vitest.
- Prefer Vue Testing Library patterns for user-centric behavior when possible.
- Do not rely on brittle DOM structure; assert on user-visible outcomes.

### 3.2 Test types
- Component tests:
    - forms (CRUD operations)
    - interactive UI behaviors
    - workflow sequences
- Store/state tests:
    - data fetching and normalization
    - error state handling
- Utility tests:
    - input validation
    - formatting utilities

### 3.3 Data-driven tests (required when appropriate)
Use parameterization when:
- validating multiple input cases
- testing transformation functions
- testing list filtering behavior
- testing error presentation for multiple backend error codes

Preferred patterns:
- `it.each([...cases])("...", (tc) => { ... })`
- Or a loop over cases with a shared helper.

### 3.4 Mocking strategy
Mock:
- backend API calls (never call real backend in unit tests unless explicitly building an integration test suite)
- browser APIs not supported in test environment
- time where relevant

For API calls:
- Provide a thin API client module and mock it in tests.
- Ensure errors and status codes map to user-visible messages and states.

### 3.5 UI assertions
Prefer:
- visible text
- aria roles/labels
- button enabled/disabled state
- emitted actions (e.g., API calls made with correct payload)

Avoid:
- asserting implementation details of UI libraries
- deep snapshot tests for large components (allowed only for small stable components)

### 3.6 Running tests
- `npm run test` runs non-watch suite (if present).
- Watch mode is required:
    - `npm run test:watch`
    - Root target `make test-frontend-watch` runs this inside docker dev workflow.

### 3.7 Auto-unmount
All frontend test files must use `enableAutoUnmount(afterEach)` from `@vue/test-utils`, either in each test file or in a shared vitest setup file. This prevents test interference from stale event listeners when `wrapper.unmount()` is not called explicitly.

### 3.8 localStorage isolation
A global `beforeEach` in the vitest setup file must call `localStorage.clear()` to prevent cross-test contamination. Do not rely on individual test files to clear localStorage.

### 3.9 Nested component mock ordering
When a parent and child component both call the same API on mount, the test mock setup must account for both calls in the correct order. Document the expected call sequence in test comments when mock ordering is non-obvious.

### 3.10 Naive UI prop casing
Naive UI template attributes use kebab-case (e.g., `consistent-menu-width`) but Vue props in test assertions use camelCase (e.g., `consistentMenuWidth`). Always use camelCase when asserting on component props in tests.

### 3.11 Security Audit
If `npm audit` shows any high-severity vulnerabilities, fail the QA cycle and have the developer upgrade the vulnerable packages.

## 4) Cross-cutting test policies

### 4.1 Fixtures and test data
- Keep fixtures small and explicit.
- Prefer builders for complex objects.
- Use deterministic IDs/timestamps via injection.

### 4.2 Logging
- Tests must not assert on full log text unless validating absence of secrets or presence of stable error codes.
- Never store real credentials in fixtures.

### 4.3 Flake policy
- No retries to mask flakiness.
- Fix flakiness at the source:
    - eliminate timing assumptions
    - isolate shared state
    - avoid global mutable singletons

### 4.4 When to add integration tests
Add targeted integration tests when:
- validating DB migrations end-to-end
- validating API routing/auth middleware for critical endpoints

Integration tests must still avoid real provider calls.

## 5) QA subagent integration

The QA expert subagent (/.claude/agents/qa-expert.md) is the final gate before a story is marked `status: uat`. It verifies:

### 5.1 Test execution
- All backend tests pass (`make test-backend`)
- All frontend tests pass (`make test-frontend`)
- Zero test failures

### 5.2 Acceptance traceability
- Each acceptance criterion in the story is traced to a specific test or verified code path
- Edge cases implied by acceptance criteria are covered
- Tests are meaningful (not trivially passing)

### 5.3 Coverage assessment
- New/changed behaviors have tests following the practices in this document
- Data-driven tests used where patterns repeat
- Happy-path and failure-path coverage for new service methods
- No real network calls in any test

### 5.4 Regression verification
- All pre-existing tests still pass
- No functionality broken by the changes

### 5.5 Application smoke test
For any story that touches backend code, the QA expert must verify the application actually starts and responds:
1. Start the application using the project's standard dev command (e.g. `make up-dev`)
2. Wait for containers/processes to be healthy
3. Verify the health endpoint returns HTTP 200 (via `curl` or equivalent)
4. Clean up (e.g. `make down`)

This catches fatal startup errors (crash loops, missing dependencies, broken wiring) that unit tests cannot detect. If the application is not reachable or returns non-200 on the health endpoint, the story fails QA.

Notes on sandbox connectivity:
- When running inside the claude-sandbox, `localhost` ports mapped to the host are not directly reachable. Use `docker compose exec <service> wget -qO- http://localhost:<port>/health` or similar within-network commands for smoke tests.
- The canonical health check path is `/health` (direct backend endpoint).

### 5.6 API endpoint verification
For stories that add or modify backend API endpoints, the QA expert must verify affected endpoints against the running application (started in 5.5):
1. For each endpoint touched by the story, issue a basic request (via `curl` from within the docker network or from the host)
2. Verify the response status code is correct (200 for success paths, appropriate 4xx/5xx for error paths)
3. Verify the response body shape matches expectations (valid JSON, expected top-level fields present)
4. This is not a substitute for unit tests — it validates that the full stack (routing, middleware, serialization, service wiring) works end-to-end

The QA expert may return a story to `in_progress` with specific issues recorded in the story's `review_feedback` field. The fullstack engineer must address all `blocker` and `important` severity issues before re-submitting.

### 5.7 Runtime error sweep (secondary findings)

After completing sections 5.5 (smoke test) and 5.6 (API endpoint verification), the QA expert must perform a runtime error sweep on the application logs. This sweep is a **secondary check** — its results do NOT block the current story. If the story's acceptance criteria pass, the story is APPROVED regardless of sweep findings.

#### 5.7.1 Procedure

1. While the application is still running (started in 5.5), capture logs:
   ```
   docker compose logs --tail=500 --no-color 2>&1
   ```
2. Filter for error-level and fatal-level messages:
   ```
   grep -iE 'level=(error|fatal|panic)|FATAL|PANIC|panic:'
   ```
3. Read `/agent/QA_ALLOWED_ERRORS.md` for the expected error allowlist.
4. For each error line found, classify it as one of:
   - **Expected**: Matches a pattern in `QA_ALLOWED_ERRORS.md`. Skip it.
   - **Bug**: An unexpected runtime error that indicates a defect. Report as a new bug ticket.
   - **Improvement**: A non-critical issue suggesting a code improvement (e.g., noisy logging for recoverable conditions). Report as an improvement idea for the appropriate file under `/agent/ideas/`.
5. Report findings in the QA verdict using the structured format (see qa-expert.md).

#### 5.7.2 Bug ticket fields

For each finding classified as **Bug**, the QA agent reports:
- A brief title (suitable for a backlog ticket)
- The error log line(s) that triggered the finding
- A brief root cause hypothesis (1-2 sentences)
- Suggested acceptance criteria (1-3 items)
- Suggested testing commands
- Suggested priority (default: 70)

#### 5.7.3 Non-blocking rule

Sweep findings never affect the story verdict. If the story's acceptance criteria pass, the story is **APPROVED** and sweep findings are filed as separate tickets by the orchestrator. This prevents infinite loops where every story bounces for pre-existing issues.

### 5.8 E2E test execution (Playwright)

The QA expert must run the full Playwright E2E suite as part of every verification cycle:

```
make test-e2e
```

Key facts:
- `make test-e2e` is fully self-contained: it starts backend + frontend using `docker-compose.e2e.yml` with `test-fixtures/` data, runs all Playwright tests, and tears down automatically. No separate `make up-dev` is required.
- The E2E compose project (`checkpoint-sampler-e2e`) is isolated from the dev environment — running `make test-e2e` does not interfere with an active `make up-dev` session.
- Results (tests run, passed, failed) must be recorded in the "E2E Test Results" section of the QA verdict.

**When E2E failures block the story:**
- If the story explicitly adds or modifies E2E tests (i.e., it is an E2E story), failures are blocking and the story must be returned to `in_progress`.

**When E2E failures are non-blocking:**
- For all other stories, E2E failures are non-blocking. Record the results, note any failures and whether they appear pre-existing, but do not reject the story on E2E failures alone. Report unexpected failures as bug tickets via the runtime error sweep mechanism.

**Frontend-only stories:**
- For stories that touch only the frontend, the E2E smoke test results from `make test-e2e` may serve as the primary smoke test, replacing the manual `make up-dev` + curl health check approach. A passing E2E smoke test confirms the full stack starts and serves requests end-to-end.

## 6) End-to-End (E2E) testing practices — Playwright

### 6.1 Frameworks and tooling
- Use `@playwright/test` (installed as a dev dependency in `/frontend`).
- Configuration lives at `frontend/playwright.config.ts`.
- E2E test files live at `frontend/e2e/` with the `*.spec.ts` extension.
- Run with `make test-e2e` — fully self-contained, no separate stack required.

### 6.2 Environment
- Base URL: `http://frontend:3000` (Vite dev server inside the E2E compose stack).
- Use headless Chromium. In the claude-sandbox, `chromiumSandbox: false` and `--no-sandbox` args are required.
- `make test-e2e` is self-contained: it starts backend + frontend using `docker-compose.e2e.yml` with
  `test-fixtures/` data, runs Playwright, then tears down. No separate `make up-dev` needed.
- The E2E compose project is named `checkpoint-sampler-e2e` — isolated from `make up-dev`.
- Test fixture data lives in `test-fixtures/`: deterministic checkpoint and sample data for reproducible E2E tests.
- Use `make up-test` / `make down-test` for an isolated test stack with separate Docker volumes and a
  separate project name (`checkpoint-sampler-test`). Resetting with `make down-test` does not affect
  the `make up-dev` environment.

### 6.3 What belongs in E2E tests
- Critical user journeys that span the full stack (UI, API, DB).
- Smoke tests that verify the app loads and the backend is reachable.
- Flows that cannot be validated by unit or component tests alone (navigation, real API responses).

### 6.4 What does NOT belong in E2E tests
- Business logic that is already covered by unit or component tests.
- Exhaustive validation of every input combination (use unit tests for that).
- Anything requiring network calls outside the project (all external calls must be internal to the test stack).

### 6.5 Writing E2E tests
- Use `page.goto('/')` and Playwright locators; prefer role-based or `data-testid` selectors.
- Use `request.get('/health')` (via the `request` fixture) to verify API endpoints through the proxy.
- Avoid arbitrary `page.waitForTimeout()` calls; use `expect(...).toBeVisible()` and Playwright's auto-waiting instead.
- Keep each spec file focused on a single feature or user journey.
- The smoke test in `frontend/e2e/smoke.spec.ts` is the canonical example.

**AC-to-test traceability**
Tests should include inline comments linking to the acceptance criteria they verify. Format: `// AC: <acceptance criterion text or summary>`. This makes QA traceability verification faster. Both unit tests and E2E tests should follow this pattern.

### 6.6 Running E2E tests
- Agent workflow (one-shot): `make test-e2e` (self-contained; starts the stack, runs tests, tears down).
- Playwright browsers are installed in the project's local `node_modules` — no global install needed.
- Playwright outputs results in list reporter format for easy log scanning.

### 6.7 Selector stability
Never use Naive UI internal CSS classes (e.g., `.n-dynamic-tags__add`, `.n-select-option`) in E2E selectors — these can change between library versions. Always use `data-testid` attributes on interactive elements. When wrapping Naive UI components, add `data-testid` to the wrapper's interactive sub-elements.

### 6.8 E2E timeout configuration
Set an explicit `timeout` in `playwright.config.ts` (e.g., `timeout: 15000`) so the value is visible and intentional. Do not rely on Playwright's default timeout.

### 6.9 NDrawer mask interaction
NDrawer renders a mask overlay that intercepts pointer events on elements behind it. In E2E tests, always close or collapse the drawer before clicking on grid cells or other elements underneath it. Add a brief `page.waitForTimeout(300)` after closing to allow the mask animation to complete.

### 6.10 Playback test timing
For timing-sensitive E2E assertions (e.g., slider playback advancement), set playback speed to the minimum value and use generous timeouts (e.g., `{ timeout: 5000 }` for a 0.25s speed gives a 20x safety margin). Avoid `page.waitForTimeout()` except for hold-position verification where you need to assert the slider has NOT moved.

## 7) Definition of Done (testing)
A story may be set to `status: uat` only when:
- New/changed behavior is covered by tests following these practices.
- Tests are deterministic and fast enough for watch workflows.
- All relevant suites pass locally and in docker watch mode (when applicable).
- The QA expert subagent has approved the story (see section 5).
