# TEST_PRACTICES.md 

This document defines how tests are written, organized, and run across backend (Go) and frontend (Vue/TypeScript). It is a contract: stories are not “done” unless tests meet these practices.

## 1) Principles

### 1.1 Tests are a product requirement
- Tests encode acceptance criteria.
- A story is incomplete without tests unless the story explicitly waives them.

### 1.2 Determinism
- Tests must be deterministic and fast.
- No sleeps as synchronization. Use proper coordination (channels, wait groups, fake clocks).

### 1.3 No real network / no real sends
- Unit tests must not call external networks.
- No test may make real external calls.
- Provider integrations are mocked/stubbed.

### 1.4 Data-driven tests by default when repeated patterns exist
- Prefer table-driven tests to reduce duplication and improve coverage.
- Backend: Ginkgo `DescribeTable` / `Entry`
- Frontend: `it.each` / parameterized helper loops

### 1.5 Test pyramid
- Unit tests: majority
- Service/API tests: targeted and meaningful
- UI/component tests: focused on behavior and critical paths

## 2) Backend (Go) testing practices

### 2.1 Frameworks and tooling
- Use Ginkgo + Gomega.
- Use race detection where feasible (`--race`).
- Use mocks generated by mockery for interfaces.
- Avoid heavy integration dependencies in unit tests.

### 2.2 Test layout and naming
- Place tests adjacent to packages under:
    - `backend/internal/...`
    - `backend/pkg/...` (if used)
    - `backend/cmd/...` (rare; keep minimal)
- Name files `*_test.go`.
- Organize specs with consistent structure:
    - `Describe("<subject>")`
    - `Context("<scenario>")`
    - `It("does <behavior>")`

### 2.3 Preferred test structure (arrange/act/assert)
- Arrange:
    - Build inputs and fakes.
    - Set up store/service with mocks.
- Act:
    - Call the unit under test.
- Assert:
    - Use Gomega expectations.
    - Verify returned values and side effects.
    - Verify mock calls when behavior depends on collaborators.

### 2.4 Data-driven tests (required when appropriate)
Use `DescribeTable` when:
- Testing validation rules over multiple inputs
- Testing error mapping or status transitions
- Testing selection logic across multiple cases

Example pattern (illustrative):
- `DescribeTable("validates inputs", func(tc testCase) { ... }, Entry("case", tc1), Entry("case", tc2))`

### 2.5 What to mock vs what to test “for real”
Mock:
- External provider clients
- Time
- Randomness / UUID generation
- External IO beyond the database (filesystems, OS calls)

Test for real:
- Domain logic in `internal/service`
- Mapping logic between domain model and persistence entities
- Schema constraints and migrations (with temporary DB files)

### 2.6 Database testing guidelines
- Use temporary DB files or in-memory databases only when they reflect production behavior.
- Prefer temp file DB for migration/schema tests to match real usage.
- Ensure tests clean up resources (file deletion).
- Verify critical constraints:
    - foreign keys (if enabled)
    - unique indexes
    - timestamps and default behaviors

### 2.7 API and transport tests (Goa)
- Test the service layer first; transport tests are secondary.
- For transport:
    - Validate status codes and response shapes for key endpoints.
    - Ensure auth gating is enforced.
    - Ensure errors are mapped to stable error codes/messages.

### 2.8 Security tests
If authentication or encryption is implemented:
- Password hashing verification (correct password succeeds, wrong fails)
- Encryption round-trip (encrypt then decrypt yields original, wrong key fails)
- Ensure secrets do not appear in logs (where log output is testable)

### 2.9 Error handling tests
- Every exported service method must have at least:
    - a happy-path test
    - a failure-path test
- Error objects should expose stable error codes for UI consumption; verify codes.

### 2.10 Running tests
Baseline expectations:
- `make test` runs the backend test suite.
- Watch mode:
    - `make test-backend-watch` runs `ginkgo watch` (race where feasible).
- CI-like run (when available):
    - `ginkgo -r --race ./internal/... ./pkg/... ./cmd/...`

### 2.11 Coverage expectations
- No hard numeric coverage gate is required initially.
- Practically:
    - service layer and schema logic should have high coverage
    - UI routing glue and generated code are excluded
- Any uncovered critical logic must be justified in the story notes.

## 3) Frontend (Vue + TypeScript) testing practices

### 3.1 Frameworks and tooling
- Use Vitest.
- Prefer Vue Testing Library patterns for user-centric behavior when possible.
- Do not rely on brittle DOM structure; assert on user-visible outcomes.

### 3.2 Test types
- Component tests:
    - forms (CRUD operations)
    - interactive UI behaviors
    - workflow sequences
- Store/state tests:
    - data fetching and normalization
    - error state handling
- Utility tests:
    - input validation
    - formatting utilities

### 3.3 Data-driven tests (required when appropriate)
Use parameterization when:
- validating multiple input cases
- testing transformation functions
- testing list filtering behavior
- testing error presentation for multiple backend error codes

Preferred patterns:
- `it.each([...cases])("...", (tc) => { ... })`
- Or a loop over cases with a shared helper.

### 3.4 Mocking strategy
Mock:
- backend API calls (never call real backend in unit tests unless explicitly building an integration test suite)
- browser APIs not supported in test environment
- time where relevant

For API calls:
- Provide a thin API client module and mock it in tests.
- Ensure errors and status codes map to user-visible messages and states.

### 3.5 UI assertions
Prefer:
- visible text
- aria roles/labels
- button enabled/disabled state
- emitted actions (e.g., API calls made with correct payload)

Avoid:
- asserting implementation details of UI libraries
- deep snapshot tests for large components (allowed only for small stable components)

### 3.6 Running tests
- `npm run test` runs non-watch suite (if present).
- Watch mode is required:
    - `npm run test:watch`
    - Root target `make test-frontend-watch` runs this inside docker dev workflow.

## 4) Cross-cutting test policies

### 4.1 Fixtures and test data
- Keep fixtures small and explicit.
- Prefer builders for complex objects.
- Use deterministic IDs/timestamps via injection.

### 4.2 Logging
- Tests must not assert on full log text unless validating absence of secrets or presence of stable error codes.
- Never store real credentials in fixtures.

### 4.3 Flake policy
- No retries to mask flakiness.
- Fix flakiness at the source:
    - eliminate timing assumptions
    - isolate shared state
    - avoid global mutable singletons

### 4.4 When to add integration tests
Add targeted integration tests when:
- validating DB migrations end-to-end
- validating API routing/auth middleware for critical endpoints

Integration tests must still avoid real provider calls.

## 5) QA subagent integration

The QA expert subagent (/.claude/agents/qa-expert.md) is the final gate before a story is marked `status: done`. It verifies:

### 5.1 Test execution
- All backend tests pass (`make test-backend`)
- All frontend tests pass (`make test-frontend`)
- Zero test failures

### 5.2 Acceptance traceability
- Each acceptance criterion in the story is traced to a specific test or verified code path
- Edge cases implied by acceptance criteria are covered
- Tests are meaningful (not trivially passing)

### 5.3 Coverage assessment
- New/changed behaviors have tests following the practices in this document
- Data-driven tests used where patterns repeat
- Happy-path and failure-path coverage for new service methods
- No real network calls in any test

### 5.4 Regression verification
- All pre-existing tests still pass
- No functionality broken by the changes

### 5.5 Application smoke test
For any story that touches backend code, the QA expert must verify the application actually starts and responds:
1. Start the application using the project's standard dev command (e.g. `make up-dev`)
2. Wait for containers/processes to be healthy
3. Verify the health endpoint returns HTTP 200 (via `curl` or equivalent)
4. Clean up (e.g. `make down`)

This catches fatal startup errors (crash loops, missing dependencies, broken wiring) that unit tests cannot detect. If the application is not reachable or returns non-200 on the health endpoint, the story fails QA.

### 5.6 API endpoint verification
For stories that add or modify backend API endpoints, the QA expert must verify affected endpoints against the running application (started in 5.5):
1. For each endpoint touched by the story, issue a basic request (via `curl` from within the docker network or from the host)
2. Verify the response status code is correct (200 for success paths, appropriate 4xx/5xx for error paths)
3. Verify the response body shape matches expectations (valid JSON, expected top-level fields present)
4. This is not a substitute for unit tests — it validates that the full stack (routing, middleware, serialization, service wiring) works end-to-end

The QA expert may return a story to `in_progress` with specific issues recorded in the story's `review_feedback` field. The fullstack engineer must address all `blocker` and `important` severity issues before re-submitting.

## 6) Definition of Done (testing)
A story may be set to `status: done` only when:
- New/changed behavior is covered by tests following these practices.
- Tests are deterministic and fast enough for watch workflows.
- All relevant suites pass locally and in docker watch mode (when applicable).
- The QA expert subagent has approved the story (see section 5).
